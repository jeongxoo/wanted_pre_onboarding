{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e3f291",
   "metadata": {},
   "source": [
    "# 문제 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe2959a",
   "metadata": {},
   "source": [
    "### **문제 1) Tokenizer 생성하기**\n",
    "\n",
    "**1-1. `preprocessing()`**\n",
    "\n",
    "텍스트 전처리를 하는 함수입니다.\n",
    "\n",
    "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
    "- output: 각 문장을 토큰화한 결과로, nested list 형태입니다. ex) [['i', 'go', 'to', 'school'], ['i', 'like', 'pizza']]\n",
    "- 조건 1: 입력된 문장에 대해서 소문자로의 변환과 특수문자 제거를 수행합니다.\n",
    "- 조건 2: 토큰화는 white space 단위로 수행합니다.\n",
    "    \n",
    "    \n",
    "\n",
    "**1-2. `fit()`**\n",
    "\n",
    "어휘 사전을 구축하는 함수입니다.\n",
    "\n",
    "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
    "- 조건 1: 위에서 만든 `preprocessing` 함수를 이용하여 각 문장에 대해 토큰화를 수행합니다.\n",
    "- 조건 2: 각각의 토큰을 정수 인덱싱 하기 위한 어휘 사전(`self.word_dict`)을 생성합니다.\n",
    "    - 주어진 코드에 있는 `self.word_dict`를 활용합니다.\n",
    "    \n",
    "\n",
    "**1-3. `transform()`**\n",
    "\n",
    "어휘 사전을 활용하여 입력 문장을 정수 인덱싱하는 함수입니다.\n",
    "\n",
    "- input: 여러 영어 문장이 포함된 list입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
    "- output: 각 문장의 정수 인덱싱으로, nested list 형태입니다. ex) [[1, 2, 3, 4], [1, 5, 6]]\n",
    "- 조건 1: 어휘 사전(`self.word_dict`)에 없는 단어는 'oov'의 index로 변환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0dd56",
   "metadata": {},
   "source": [
    "## 함수 해석\n",
    "### preprocessing()\n",
    "- 화이트스페이스 토크나이징\n",
    "- lower case\n",
    "\n",
    "### fit()\n",
    "- 모든 단어에 대해 워드 딕셔너리 생성\n",
    "- task 종료 후 fit_checker = True로 설정하여\n",
    "- transform()이 작동하도록 설정\n",
    "\n",
    "### transform()\n",
    "- word_dict이 생성되어 있어야 정수 인덱싱 가능\n",
    "- 따라서 fit_checker를 통해 word_dict 생성 task의 완료 여부 확인\n",
    "- task가 수행되었다면, word_dict 기반 정수 인덱싱\n",
    "- task가 수행되지 않았다면, return\n",
    "- 개선 방향\n",
    "    - 형태소 분석기를 사용하여 각 단어를 원형으로 토크나이징한 후 word_dict 생성\n",
    "\n",
    "### fit_transform()\n",
    "- fit(), transform()의 순차 실행\n",
    "- fit()\n",
    "    - word_dict 생성 및 fit_check 설정\n",
    "- transform()\n",
    "    - fit_check에 따라 정수 인덱싱 or error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d27eb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import os\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28aeaaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        self.word_dict = {'oov': 0}\n",
    "        self.fit_checker = False\n",
    "\n",
    "    def preprocessing(self, sequences):\n",
    "#         return [list(map(lambda x: x.lower, list(filter(str.isalnum, s)))) for s in sequences]\n",
    "        return [list(map(lambda x: re.sub(r\"[^a-zA-Z0-9]\", \"\", x).lower(), s.split())) for s in sequences]\n",
    "  \n",
    "    def fit(self, sequences):\n",
    "        self.fit_checker = False\n",
    "        tokens = self.preprocessing(sequences)\n",
    "        tokens = set(reduce(lambda x, y: x + y, tokens))\n",
    "        self.word_dict = dict(self.word_dict, \n",
    "                              **{v:i+1 for i,v in enumerate(tokens)}) \n",
    "        self.fit_checker = True\n",
    "    \n",
    "    def transform(self, sequences):\n",
    "        result = []\n",
    "        tokens = self.preprocessing(sequences)\n",
    "        if self.fit_checker:\n",
    "            result = [list(map(lambda x: self.word_dict[x if x in self.word_dict.keys() else \"oov\"], t)) \n",
    "                      for t in tokens]\n",
    "            return result\n",
    "        else:\n",
    "            raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
    "      \n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        result = self.transform(sequences)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fab8a4fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open(os.path.join(\"test_data.txt\"), \"r\")\n",
    "text = f.read()\n",
    "text = text.split(\".\")\n",
    "text = [t.replace(\"\\n\", \"\").strip() for t in text]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02108353",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b82e9b6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tk = Tokenizer()\n",
    "start = time.time()\n",
    "pre = tk.preprocessing(sequences=text)\n",
    "# for p in pre: print(p, \"\\n\")\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ad78ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 0.001150369644165039\n"
     ]
    }
   ],
   "source": [
    "print(\"time :\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79dd5054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006586859226226806\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "tk = Tokenizer()\n",
    "for _ in range(1000):\n",
    "    start = time.time()\n",
    "    pre = tk.preprocessing(sequences=text)\n",
    "    end = time.time()\n",
    "    times.append(end-start)\n",
    "    \n",
    "print(sum(times)/len(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0b6245e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.018002724647522\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "tk = Tokenizer()\n",
    "for _ in range(10):\n",
    "    start = time.time()\n",
    "    pre = tk.preprocessing(sequences=text)\n",
    "    end = time.time()\n",
    "    times.append(end-start)\n",
    "    \n",
    "print(sum(times)/len(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52dfed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = [[\"1\", \"2\"], [\"3\", \"4\", \"1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d680df2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for s in set(reduce(lambda x, y: x + y, tt)):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c034c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "*ttt, = tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88c4c38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '2'], ['3', '4', '1']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "141e8574",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5a078e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [0]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.transform([\"dasds\", \"12313\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03894a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'oov': 0,\n",
       " 'bypassing': 1,\n",
       " 'effective': 2,\n",
       " 'ai': 3,\n",
       " 'equipment': 4,\n",
       " 'used': 5,\n",
       " 'dimensions': 6,\n",
       " 'direct': 7,\n",
       " 'machines': 8,\n",
       " 'central': 9,\n",
       " 'including': 10,\n",
       " 'includes': 11,\n",
       " '23': 12,\n",
       " 'dream': 13,\n",
       " 'it': 14,\n",
       " 'device': 15,\n",
       " 'programs': 16,\n",
       " 'october': 17,\n",
       " 'very': 18,\n",
       " 'sent': 19,\n",
       " 'the': 20,\n",
       " 'oped': 21,\n",
       " 'specification': 22,\n",
       " 'from': 23,\n",
       " 'parts': 24,\n",
       " '32feet': 25,\n",
       " 'nothing': 26,\n",
       " 'wave': 27,\n",
       " 'provided': 28,\n",
       " 'alternately': 29,\n",
       " 'or': 30,\n",
       " 'architecture': 31,\n",
       " 'digital': 32,\n",
       " 'dartmouth': 33,\n",
       " 'initially': 34,\n",
       " 'transistors': 35,\n",
       " 'handle': 36,\n",
       " 'andy': 37,\n",
       " 'example': 38,\n",
       " 'except': 39,\n",
       " 'operation': 40,\n",
       " 'a': 41,\n",
       " 'analogue': 42,\n",
       " 'topic': 43,\n",
       " 'interaction': 44,\n",
       " 'computer': 45,\n",
       " 'develops': 46,\n",
       " 'any': 47,\n",
       " 'founded': 48,\n",
       " 'electronic': 49,\n",
       " 'exhibit': 50,\n",
       " 'has': 51,\n",
       " 'many': 52,\n",
       " 'right': 53,\n",
       " 'where': 54,\n",
       " 'unit': 55,\n",
       " 'allow': 56,\n",
       " '2005': 57,\n",
       " 'communication': 58,\n",
       " 'evaluates': 59,\n",
       " 'this': 60,\n",
       " 'see': 61,\n",
       " 'keyboard': 62,\n",
       " 'as': 63,\n",
       " 'mobile': 64,\n",
       " 'mbps': 65,\n",
       " 'technology': 66,\n",
       " 'off': 67,\n",
       " 'together': 68,\n",
       " 'was': 69,\n",
       " '2008': 70,\n",
       " '5': 71,\n",
       " 'process': 72,\n",
       " 'processing': 73,\n",
       " 'later': 74,\n",
       " 'power': 75,\n",
       " 'high': 76,\n",
       " 'computing': 77,\n",
       " 'web': 78,\n",
       " 'short': 79,\n",
       " 'frequency': 80,\n",
       " 'pages': 81,\n",
       " 'ghz': 82,\n",
       " 'value': 83,\n",
       " 'layout': 84,\n",
       " 'task': 85,\n",
       " 'coined': 86,\n",
       " 'represented': 87,\n",
       " 'discussed': 88,\n",
       " 'current': 89,\n",
       " 'backend': 90,\n",
       " 'network': 91,\n",
       " 'low': 92,\n",
       " 'them': 93,\n",
       " 'general': 94,\n",
       " 'htc': 95,\n",
       " 'design': 96,\n",
       " 'referred': 97,\n",
       " 'rubin': 98,\n",
       " '4': 99,\n",
       " 'describe': 100,\n",
       " 'in': 101,\n",
       " '1955': 102,\n",
       " 'location': 103,\n",
       " 'sends': 104,\n",
       " 'user': 105,\n",
       " 'describes': 106,\n",
       " 'numerous': 107,\n",
       " 'running': 108,\n",
       " 'iphone': 109,\n",
       " 'strong': 110,\n",
       " 'turing': 111,\n",
       " 'hope': 112,\n",
       " 'range': 113,\n",
       " 'intelligence': 114,\n",
       " 'throughput': 115,\n",
       " 'use': 116,\n",
       " 'behavio': 117,\n",
       " 'know': 118,\n",
       " 'industry': 119,\n",
       " 'is': 120,\n",
       " 'stores': 121,\n",
       " 'operating': 122,\n",
       " 'applications': 123,\n",
       " 'moving': 124,\n",
       " 'john': 125,\n",
       " 'public': 126,\n",
       " 'piece': 127,\n",
       " 'at': 128,\n",
       " 'artificial': 129,\n",
       " 'considered': 130,\n",
       " 'nonprofit': 131,\n",
       " 'been': 132,\n",
       " 'all': 133,\n",
       " 'usually': 134,\n",
       " 'components': 135,\n",
       " 'rf': 136,\n",
       " 'tmobiles': 137,\n",
       " 'alternatively': 138,\n",
       " 'archives': 139,\n",
       " 'apart': 140,\n",
       " 'seapeayou': 141,\n",
       " 'continuously': 142,\n",
       " 'remote': 143,\n",
       " 'based': 144,\n",
       " '17': 145,\n",
       " 'can': 146,\n",
       " 'deals': 147,\n",
       " 'free': 148,\n",
       " 'presented': 149,\n",
       " 'file': 150,\n",
       " 'ccess': 151,\n",
       " 'acquired': 152,\n",
       " 'cloud': 153,\n",
       " 'rival': 154,\n",
       " 'insert': 155,\n",
       " 'storage': 156,\n",
       " 'texts': 157,\n",
       " 'develop': 158,\n",
       " 'abstract': 159,\n",
       " 'that': 160,\n",
       " 'analog': 161,\n",
       " 'arranged': 162,\n",
       " 'well': 163,\n",
       " 'visitors': 164,\n",
       " 'human': 165,\n",
       " 'study': 166,\n",
       " 'designs': 167,\n",
       " 'interact': 168,\n",
       " 'who': 169,\n",
       " 'processes': 170,\n",
       " 'class': 171,\n",
       " 'with': 172,\n",
       " 'to': 173,\n",
       " 'rate': 174,\n",
       " 'devices': 175,\n",
       " 'and': 176,\n",
       " 'originally': 177,\n",
       " 'computers': 178,\n",
       " 'such': 179,\n",
       " '1950': 180,\n",
       " 'transmission': 181,\n",
       " 'assistants': 182,\n",
       " 'contains': 183,\n",
       " 'first': 184,\n",
       " 'gain': 185,\n",
       " 'microprocessor': 186,\n",
       " 'up': 187,\n",
       " 'information': 188,\n",
       " 'compressed': 189,\n",
       " 'scientific': 190,\n",
       " 'area': 191,\n",
       " 'signals': 192,\n",
       " 'only': 193,\n",
       " 'their': 194,\n",
       " 'for': 195,\n",
       " 'one': 196,\n",
       " 'may': 197,\n",
       " 'languages': 198,\n",
       " 'ability': 199,\n",
       " 'tasks': 200,\n",
       " 'manhole': 201,\n",
       " 'handles': 202,\n",
       " 'aims': 203,\n",
       " 'like': 204,\n",
       " 'open': 205,\n",
       " 'learn': 206,\n",
       " 'security': 207,\n",
       " 'mouse': 208,\n",
       " 'old': 209,\n",
       " 'college': 210,\n",
       " 'access': 211,\n",
       " 'amount': 212,\n",
       " 'meters': 213,\n",
       " 'downloaded': 214,\n",
       " 'background': 215,\n",
       " 'archive': 216,\n",
       " 'websites': 217,\n",
       " 'overall': 218,\n",
       " 'software': 219,\n",
       " 'hardware': 220,\n",
       " 'be': 221,\n",
       " 'password': 222,\n",
       " 'secure': 223,\n",
       " 'devel': 224,\n",
       " 'cpu': 225,\n",
       " 'development': 226,\n",
       " 'servers': 227,\n",
       " 'now': 228,\n",
       " 'restricted': 229,\n",
       " 'changes': 230,\n",
       " 'collection': 231,\n",
       " 'processor': 232,\n",
       " 'august': 233,\n",
       " 'perform': 234,\n",
       " 'they': 235,\n",
       " 'test': 236,\n",
       " 'november': 237,\n",
       " 'far': 238,\n",
       " 'google': 239,\n",
       " 'data': 240,\n",
       " 'allows': 241,\n",
       " 'backdoor': 242,\n",
       " 'more': 243,\n",
       " 'linux': 244,\n",
       " 'responsible': 245,\n",
       " 'inbetween': 246,\n",
       " 'trapdoor': 247,\n",
       " 'communicate': 248,\n",
       " 'stack': 249,\n",
       " 'referring': 250,\n",
       " 'files': 251,\n",
       " 'developer': 252,\n",
       " 'platform': 253,\n",
       " 'operate': 254,\n",
       " 'apple': 255,\n",
       " 'backing': 256,\n",
       " 'transfer': 257,\n",
       " 'closed': 258,\n",
       " 'etc': 259,\n",
       " '1': 260,\n",
       " 'research': 261,\n",
       " 'photographic': 262,\n",
       " 'person': 263,\n",
       " 'browsed': 264,\n",
       " 'are': 265,\n",
       " 'thats': 266,\n",
       " 'instead': 267,\n",
       " 'also': 268,\n",
       " 'al': 269,\n",
       " 'phone': 270,\n",
       " 'without': 271,\n",
       " 'site': 272,\n",
       " 'other': 273,\n",
       " '10': 274,\n",
       " '2': 275,\n",
       " 'accessed': 276,\n",
       " 'on': 277,\n",
       " 'method': 278,\n",
       " 'september': 279,\n",
       " 'instructions': 280,\n",
       " 'services': 281,\n",
       " 'multiple': 282,\n",
       " 'server': 283,\n",
       " 'bluetooth': 284,\n",
       " 'would': 285,\n",
       " 'which': 286,\n",
       " 'using': 287,\n",
       " 'internetconnected': 288,\n",
       " 'indistinguishable': 289,\n",
       " '2003': 290,\n",
       " 'picture': 291,\n",
       " 'users': 292,\n",
       " 'scripts': 293,\n",
       " 'being': 294,\n",
       " 'emulates': 295,\n",
       " 'released': 296,\n",
       " 'change': 297,\n",
       " 'vast': 298,\n",
       " 'browser': 299,\n",
       " 'code': 300,\n",
       " '1996': 301,\n",
       " 'mccarthy': 302,\n",
       " '721': 303,\n",
       " 'audio': 304,\n",
       " 'system': 305,\n",
       " 'another': 306,\n",
       " 'g1': 307,\n",
       " '2007': 308,\n",
       " 'pronounced': 309,\n",
       " 'varying': 310,\n",
       " 'values': 311,\n",
       " 'work': 312,\n",
       " 'connections': 313,\n",
       " 'voltage': 314,\n",
       " 'developed': 315,\n",
       " 'middleware': 316,\n",
       " 'images': 317,\n",
       " 'have': 318,\n",
       " 'performed': 319,\n",
       " 'signal': 320,\n",
       " 'massive': 321,\n",
       " 'operates': 322,\n",
       " 'kbps': 323,\n",
       " 'how': 324,\n",
       " 'frontend': 325,\n",
       " 'over': 326,\n",
       " 'an': 327,\n",
       " 'receives': 328,\n",
       " 'by': 329,\n",
       " 'ios': 330,\n",
       " 'since': 331,\n",
       " 'programmer': 332,\n",
       " 'smartphones': 333,\n",
       " 'platforms': 334,\n",
       " 'distributed': 335,\n",
       " 'programming': 336,\n",
       " 'provides': 337,\n",
       " 'doesnt': 338,\n",
       " 'internet': 339,\n",
       " 'android': 340,\n",
       " 'depending': 341,\n",
       " 'part': 342,\n",
       " 'hidden': 343,\n",
       " 'include': 344,\n",
       " 'of': 345,\n",
       " 'term': 346,\n",
       " 'each': 347,\n",
       " 'about': 348,\n",
       " 'awareness': 349,\n",
       " 'similar': 350,\n",
       " 'alan': 351,\n",
       " 'when': 352,\n",
       " 'related': 353,\n",
       " 'placed': 354,\n",
       " 'telecommunications': 355,\n",
       " 'requivalent': 356,\n",
       " 'contrast': 357,\n",
       " 'brain': 358}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "83c27a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.fit(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883548d0",
   "metadata": {},
   "source": [
    "# 문제 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "199867c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfVectorizer:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fit_checker = False\n",
    "  \n",
    "    def fit(self, sequences):\n",
    "        tokenized = self.tokenizer.fit_transform(sequences)\n",
    "        '''\n",
    "        문제 2-1.\n",
    "        '''\n",
    "        self.fit_checker = True\n",
    "    \n",
    "    def transform(self, sequences):\n",
    "        if self.fit_checker:\n",
    "            tokenized = self.tokenizer.transform(sequences)\n",
    "            '''\n",
    "            문제 2-2.\n",
    "            '''\n",
    "            return self.tfidf_matrix\n",
    "        else:\n",
    "            raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
    "\n",
    "  \n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        return self.transform(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d1509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
